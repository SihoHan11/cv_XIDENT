{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccceff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본 트레이닝\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 가장 가벼운 모델 로드\n",
    "# model = YOLO('yolo26n.pt')\n",
    "\n",
    "# 마지막 에포크 로드\n",
    "model = YOLO('runs/detect/train/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.4.14 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.4.13  Python-3.11.2 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=2.0, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=f:\\Study\\cv\\cv_XIDENT\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=runs\\detect\\train\\weights\\last.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=f:\\Study\\cv\\runs\\detect, rect=False, resume=runs\\detect\\train\\weights\\last.pt, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=F:\\Study\\cv\\runs\\detect\\train, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=10, workspace=None\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir F:\\Study\\cv\\runs\\detect\\train', view at http://localhost:6006/\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       176  ultralytics.nn.modules.conv.Conv             [1, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5, 3, True]        \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    119808  ultralytics.nn.modules.block.C3k2            [384, 128, 1, True]           \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     34304  ultralytics.nn.modules.block.C3k2            [256, 64, 1, True]            \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     95232  ultralytics.nn.modules.block.C3k2            [192, 128, 1, True]           \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    463104  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True, 0.5, True]\n",
      " 23        [16, 19, 22]  1    243126  ultralytics.nn.modules.head.Detect           [5, 1, True, [64, 128, 256]]  \n",
      "YOLO26n summary: 260 layers, 2,505,462 parameters, 2,505,462 gradients, 5.7 GFLOPs\n",
      "\n",
      "Transferred 708/708 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 557.5581.0 MB/s, size: 86.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning F:\\Study\\cv\\dataset_50k\\Training\\labels.cache... 49994 images, 2500 backgrounds, 1 corrupt: 100% ━━━━━━━━━━━━ 49995/49995  0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mF:\\Study\\cv\\dataset_50k\\Training\\images\\R_314_60_M_09_M0_G1_C0_09.jpg: ignoring corrupt image/label: cannot identify image file 'F:\\\\Study\\\\cv\\\\dataset_50k\\\\Training\\\\images\\\\R_314_60_M_09_M0_G1_C0_09.jpg'\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 3060 Ti) 8.00G total, 0.10G reserved, 0.05G allocated, 7.85G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     2505462       5.721         0.440         50.21           nan        (1, 1, 640, 640)                    list\n",
      "     2505462       11.44         0.986            64           nan        (2, 1, 640, 640)                    list\n",
      "     2505462       22.89         1.734         39.83           nan        (4, 1, 640, 640)                    list\n",
      "     2505462       45.77         3.181         48.23           nan        (8, 1, 640, 640)                    list\n",
      "     2505462       91.54         5.994         71.14           nan       (16, 1, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 13 for CUDA:0 5.09G/8.00G (64%) \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 468.1311.1 MB/s, size: 79.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning F:\\Study\\cv\\dataset_50k\\Training\\labels.cache... 49994 images, 2500 backgrounds, 1 corrupt: 100% ━━━━━━━━━━━━ 49995/49995  0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mF:\\Study\\cv\\dataset_50k\\Training\\images\\R_314_60_M_09_M0_G1_C0_09.jpg: ignoring corrupt image/label: cannot identify image file 'F:\\\\Study\\\\cv\\\\dataset_50k\\\\Training\\\\images\\\\R_314_60_M_09_M0_G1_C0_09.jpg'\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.2 ms, read: 303.8176.5 MB/s, size: 83.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\Study\\cv\\dataset_50k\\Validation\\labels... 14025 images, 528 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 14025/14025 729.5it/s 19.2s<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: F:\\Study\\cv\\dataset_50k\\Validation\\labels.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.01, momentum=0.9) with parameter groups 114 weight(decay=0.0), 126 weight(decay=0.0005078125), 126 bias(decay=0.0)\n",
      "Plotting labels to F:\\Study\\cv\\runs\\detect\\train\\labels.jpg... \n",
      "Resuming training runs\\detect\\train\\weights\\last.pt from epoch 19 to 50 total epochs\n",
      "WARNING \u001b[34m\u001b[1mTensorBoard: \u001b[0mTensorBoard graph visualization failure: Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 3, 640, 640] to have 1 channels, but got 3 channels instead -> Given groups=1, weight of size [16, 1, 3, 3], expected input[1, 3, 640, 640] to have 1 channels, but got 3 channels instead\n",
      "Image sizes 640 train, 640 val\n",
      "Using 10 dataloader workers\n",
      "Logging results to \u001b[1mF:\\Study\\cv\\runs\\detect\\train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/50      2.03G      1.058      1.421   0.003817         64        640: 100% ━━━━━━━━━━━━ 3846/3846 5.5it/s 11:34<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 7.4it/s 1:12<0.1sss\n",
      "                   all      14025      49644      0.942      0.944      0.971      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/50      2.03G      1.069      1.468   0.003871         57        640: 100% ━━━━━━━━━━━━ 3846/3846 5.7it/s 11:15<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.5it/s 57.1s<0.2ss\n",
      "                   all      14025      49644      0.945      0.943       0.97       0.71\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/50      2.04G       1.07      1.463   0.003849         50        640: 100% ━━━━━━━━━━━━ 3846/3846 6.3it/s 10:14<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 10.0it/s 54.0s0.1ss\n",
      "                   all      14025      49644      0.944      0.944       0.97      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/50      2.05G      1.066      1.465   0.003858         64        640: 100% ━━━━━━━━━━━━ 3846/3846 6.4it/s 10:02<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.8it/s 55.2s<0.1ss\n",
      "                   all      14025      49644      0.944      0.944      0.971      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/50      2.07G      1.058      1.446   0.003811         47        640: 100% ━━━━━━━━━━━━ 3846/3846 6.3it/s 10:07<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 10.1it/s 53.5s0.1ss\n",
      "                   all      14025      49644      0.942      0.943      0.971      0.712\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/50      2.08G      1.052      1.444   0.003779         62        640: 100% ━━━━━━━━━━━━ 3846/3846 6.3it/s 10:07<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 10.1it/s 53.2s<0.1s\n",
      "                   all      14025      49644      0.937      0.943      0.968       0.71\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/50      2.08G      1.051      1.436   0.003771         67        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:19<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.4it/s 57.2s<0.1ss\n",
      "                   all      14025      49644      0.936      0.943      0.967      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/50      2.11G      1.045      1.413    0.00375         54        640: 100% ━━━━━━━━━━━━ 3846/3846 6.1it/s 10:30<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.7it/s 55.9s<0.1ss\n",
      "                   all      14025      49644      0.931      0.942      0.962      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/50      2.12G      1.039      1.417   0.003737         63        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:18<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.9it/s 54.8s<0.2ss\n",
      "                   all      14025      49644      0.926      0.942      0.958      0.703\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/50      2.14G      1.036      1.403   0.003724         51        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:19<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.9it/s 54.8s<0.1ss\n",
      "                   all      14025      49644      0.922      0.942      0.956      0.702\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/50      2.15G      1.029      1.389   0.003689         56        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:18<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.7it/s 55.5s<0.1ss\n",
      "                   all      14025      49644       0.92      0.944      0.958      0.704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/50      2.16G      1.026      1.386   0.003671         61        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:18<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.8it/s 55.0s<0.1ss\n",
      "                   all      14025      49644      0.921      0.944      0.959      0.704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/50      2.18G      1.019       1.37   0.003633         52        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:19<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.8it/s 54.9s<0.1ss\n",
      "                   all      14025      49644      0.925      0.944      0.961      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/50       2.2G      1.017      1.384   0.003623         46        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:17<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.9it/s 54.7ss<0.1s\n",
      "                   all      14025      49644      0.926      0.943      0.963      0.707\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/50      2.21G      1.009      1.362   0.003594         55        640: 100% ━━━━━━━━━━━━ 3846/3846 6.2it/s 10:17<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 9.9it/s 54.5s<0.1ss\n",
      "                   all      14025      49644      0.927      0.943      0.962      0.706\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 23, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "15 epochs completed in 2.840 hours.\n",
      "Optimizer stripped from F:\\Study\\cv\\runs\\detect\\train\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from F:\\Study\\cv\\runs\\detect\\train\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating F:\\Study\\cv\\runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.4.13  Python-3.11.2 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "YOLO26n summary (fused): 122 layers, 2,375,523 parameters, 0 gradients, 5.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 540/540 11.0it/s 49.1s<0.1s\n",
      "                   all      14025      49644      0.943      0.942      0.971      0.712\n",
      "            eye_opened       8156      15036      0.943      0.931      0.979       0.67\n",
      "            eye_closed       6246      11046      0.922      0.896      0.955      0.564\n",
      "          mouth_opened       5337       5337      0.964      0.907      0.972      0.717\n",
      "          mouth_closed       4736       4736      0.889      0.977      0.954      0.686\n",
      "                  face      13489      13489      0.998      0.998      0.995      0.921\n",
      "Speed: 0.0ms preprocess, 0.8ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "train_result = model.train(\n",
    "    data='cv_XIDENT/data.yaml',   # 파일럿 데이터 경로가 적힌 yaml\n",
    "    name='train_lcls',\n",
    "    epochs=50,                # 검증용이므로 짧게 설정\n",
    "    imgsz=640,\n",
    "    cls=1.5,\n",
    "    batch=-1,\n",
    "    device=0,\n",
    "    workers=10,\n",
    "    patience=10,\n",
    "    save_period=5\n",
    "    # dfl=2.0                # '맞추기 쉬운 샘플'의 비중을 줄이고 '어려운 샘플'에 집중하게 만드는 방식\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75916314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.13  Python-3.11.2 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "YOLO26n summary (fused): 122 layers, 2,375,523 parameters, 0 gradients, 5.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 630.7324.6 MB/s, size: 92.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\Study\\cv\\dataset_50k\\Validation\\labels.cache... 14025 images, 528 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 14025/14025  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 877/877 3.6it/s 4:01<0.3s\n",
      "                   all      14025      49644      0.942      0.943      0.962      0.722\n",
      "            eye_opened       8156      15036      0.941      0.933      0.968      0.688\n",
      "            eye_closed       6246      11046       0.92      0.898      0.941      0.585\n",
      "          mouth_opened       5337       5337      0.963      0.908      0.953      0.717\n",
      "          mouth_closed       4736       4736      0.888      0.977      0.951      0.699\n",
      "                  face      13489      13489      0.998      0.998      0.995      0.921\n",
      "Speed: 0.2ms preprocess, 2.4ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\val2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "\n",
    "metrics = model.val(data=\"cv_XIDENT/data.yaml\", conf=0.25, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010eed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete  (12 CPUs, 15.7 GB RAM, 225.9/237.5 GB disk)\n",
      "\n",
      "Benchmarks complete for runs/detect/train/weights/best.pt on data.yaml at imgsz=640 (3.75s)\n",
      "Benchmarks legend:  -  Success  -  Export passed but validation failed  -  Export failed\n",
      "+----------------------------------------------------------------------------------------+\n",
      "|     Format   Status   Size (MB)   metrics/mAP50-95(B)   Inference time (ms/im)   FPS |\n",
      "+========================================================================================+\n",
      "| 1   ONNX              0.0         -                     -                        -   |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><thead><tr><th> </th><th>Format</th><th>Status❔</th><th>Size (MB)</th><th>metrics/mAP50-95(B)</th><th>Inference time (ms/im)</th><th>FPS</th></tr></thead><tbody><tr><td>&quot;1&quot;</td><td>&quot;ONNX&quot;</td><td>&quot;❌&quot;</td><td>&quot;0.0&quot;</td><td>&quot;-&quot;</td><td>&quot;-&quot;</td><td>&quot;-&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "+----------------------------------------------------------------------------------------+\n",
       "|     Format   Status❔   Size (MB)   metrics/mAP50-95(B)   Inference time (ms/im)   FPS |\n",
       "+========================================================================================+\n",
       "| 1   ONNX     ❌         0.0         -                     -                        -   |\n",
       "+----------------------------------------------------------------------------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics.utils.benchmarks import benchmark\n",
    "\n",
    "# Benchmark on GPU\n",
    "# benchmark(model=\"runs/detect/train/weights/best.pt\", data=\"cv_XIDENT/data.yaml\", imgsz=640, format='', device=0)\n",
    "\n",
    "# Benchmark specific export format\n",
    "benchmark(model=\"runs/detect/train/weights/best.pt\", data=\"data.yaml\", imgsz=640, format=\"openvino\", device=\"intel:gpu\")\n",
    "benchmark(model=\"runs/detect/train/weights/best.pt\", data=\"data.yaml\", imgsz=640, format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53916744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs/detect/train/weights/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.24.1 with CUDAExecutionProvider\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\dataset_50k\\Validation\\images\\045_G1_01____20200929_110850_03518_flip.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 7.0ms\n",
      "Speed: 0.5ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\onnx_result\u001b[0m\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'eye_opened', 1: 'eye_closed', 2: 'mouth_opened', 3: 'mouth_closed', 4: 'face'}\n",
      "obb: None\n",
      "orig_img: array([[[ 13],\n",
      "        [ 34],\n",
      "        [ 46],\n",
      "        ...,\n",
      "        [ 61],\n",
      "        [ 70],\n",
      "        [  0]],\n",
      "\n",
      "       [[149],\n",
      "        [ 50],\n",
      "        [ 25],\n",
      "        ...,\n",
      "        [ 65],\n",
      "        [ 74],\n",
      "        [  2]],\n",
      "\n",
      "       [[ 63],\n",
      "        [ 44],\n",
      "        [ 38],\n",
      "        ...,\n",
      "        [ 68],\n",
      "        [ 77],\n",
      "        [  3]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  8],\n",
      "        [ 88],\n",
      "        [ 79],\n",
      "        ...,\n",
      "        [ 63],\n",
      "        [ 66],\n",
      "        [  1]],\n",
      "\n",
      "       [[  5],\n",
      "        [ 72],\n",
      "        [ 63],\n",
      "        ...,\n",
      "        [ 64],\n",
      "        [ 66],\n",
      "        [  0]],\n",
      "\n",
      "       [[  3],\n",
      "        [ 57],\n",
      "        [ 51],\n",
      "        ...,\n",
      "        [ 65],\n",
      "        [ 66],\n",
      "        [  0]]], dtype=uint8)\n",
      "orig_shape: (1280, 800)\n",
      "path: 'f:\\\\Study\\\\cv\\\\dataset_50k\\\\Validation\\\\images\\\\045_G1_01_무광원_네비게이션_하품재현_20200929_110850_03518_flip.jpg'\n",
      "probs: None\n",
      "save_dir: 'F:\\\\Study\\\\cv\\\\runs\\\\detect\\\\onnx_result'\n",
      "speed: {'preprocess': 0.5193999968469143, 'inference': 7.038199997623451, 'postprocess': 0.7449000113410875}\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([4., 2., 0., 0.], device='cuda:0')\n",
      "conf: tensor([0.9414, 0.8664, 0.7818, 0.7734], device='cuda:0')\n",
      "data: tensor([[1.7460e+02, 5.7780e+02, 4.6636e+02, 9.3891e+02, 9.4141e-01, 4.0000e+00],\n",
      "        [2.2022e+02, 7.6127e+02, 3.1503e+02, 8.7652e+02, 8.6642e-01, 2.0000e+00],\n",
      "        [1.8447e+02, 6.4096e+02, 2.3743e+02, 6.7744e+02, 7.8178e-01, 0.0000e+00],\n",
      "        [2.8221e+02, 6.1760e+02, 3.5137e+02, 6.5859e+02, 7.7344e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1280, 800)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[320.4780, 758.3560, 291.7589, 361.1048],\n",
      "        [267.6254, 818.8942,  94.8086, 115.2543],\n",
      "        [210.9512, 659.2004,  52.9674,  36.4789],\n",
      "        [316.7916, 638.0958,  69.1622,  40.9849]], device='cuda:0')\n",
      "xywhn: tensor([[0.4006, 0.5925, 0.3647, 0.2821],\n",
      "        [0.3345, 0.6398, 0.1185, 0.0900],\n",
      "        [0.2637, 0.5150, 0.0662, 0.0285],\n",
      "        [0.3960, 0.4985, 0.0865, 0.0320]], device='cuda:0')\n",
      "xyxy: tensor([[174.5985, 577.8036, 466.3574, 938.9084],\n",
      "        [220.2212, 761.2670, 315.0297, 876.5213],\n",
      "        [184.4675, 640.9609, 237.4349, 677.4399],\n",
      "        [282.2104, 617.6033, 351.3727, 658.5882]], device='cuda:0')\n",
      "xyxyn: tensor([[0.2182, 0.4514, 0.5829, 0.7335],\n",
      "        [0.2753, 0.5947, 0.3938, 0.6848],\n",
      "        [0.2306, 0.5008, 0.2968, 0.5292],\n",
      "        [0.3528, 0.4825, 0.4392, 0.5145]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# onnx predict\n",
    "model_onnx = YOLO('runs/detect/train/weights/best.onnx')\n",
    "\n",
    "source = 'dataset_50k/Validation/images/045_G1_01_무광원_네비게이션_하품재현_20200929_110850_03518_flip.jpg'\n",
    "\n",
    "onnx_result = model_onnx.predict(source, conf=0.25, save=True, exist_ok=True, name='onnx_result')\n",
    "\n",
    "# View results\n",
    "for r in onnx_result:\n",
    "    print(r)\n",
    "    print(r.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18794302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 f:\\Study\\cv\\dataset_50k\\Validation\\images\\045_G1_01____20200929_110850_03518_flip.jpg: 640x416 2 eye_openeds, 1 mouth_opened, 1 face, 23.3ms\n",
      "Speed: 2.5ms preprocess, 23.3ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 416)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\result\u001b[0m\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([4., 2., 0., 0.], device='cuda:0')\n",
      "conf: tensor([0.9392, 0.8469, 0.7656, 0.7504], device='cuda:0')\n",
      "data: tensor([[1.7546e+02, 5.7754e+02, 4.6545e+02, 9.3847e+02, 9.3920e-01, 4.0000e+00],\n",
      "        [2.2056e+02, 7.6053e+02, 3.1451e+02, 8.7732e+02, 8.4687e-01, 2.0000e+00],\n",
      "        [1.8503e+02, 6.4086e+02, 2.3805e+02, 6.7698e+02, 7.6560e-01, 0.0000e+00],\n",
      "        [2.8231e+02, 6.1719e+02, 3.5189e+02, 6.5805e+02, 7.5043e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1280, 800)\n",
      "shape: torch.Size([4, 6])\n",
      "xywh: tensor([[320.4507, 758.0068, 289.9892, 360.9344],\n",
      "        [267.5341, 818.9225,  93.9475, 116.7903],\n",
      "        [211.5392, 658.9220,  53.0154,  36.1187],\n",
      "        [317.1007, 637.6199,  69.5762,  40.8538]], device='cuda:0')\n",
      "xywhn: tensor([[0.4006, 0.5922, 0.3625, 0.2820],\n",
      "        [0.3344, 0.6398, 0.1174, 0.0912],\n",
      "        [0.2644, 0.5148, 0.0663, 0.0282],\n",
      "        [0.3964, 0.4981, 0.0870, 0.0319]], device='cuda:0')\n",
      "xyxy: tensor([[175.4561, 577.5397, 465.4453, 938.4741],\n",
      "        [220.5604, 760.5273, 314.5079, 877.3176],\n",
      "        [185.0315, 640.8627, 238.0469, 676.9813],\n",
      "        [282.3126, 617.1931, 351.8889, 658.0468]], device='cuda:0')\n",
      "xyxyn: tensor([[0.2193, 0.4512, 0.5818, 0.7332],\n",
      "        [0.2757, 0.5942, 0.3931, 0.6854],\n",
      "        [0.2313, 0.5007, 0.2976, 0.5289],\n",
      "        [0.3529, 0.4822, 0.4399, 0.5141]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# pt predict\n",
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "\n",
    "source = 'dataset_50k/Validation/images/045_G1_01_무광원_네비게이션_하품재현_20200929_110850_03518_flip.jpg'\n",
    "\n",
    "result = model.predict(source, conf=0.25, save=True, exist_ok=True, name='result')\n",
    "\n",
    "# View results\n",
    "for r in result:\n",
    "    # print(r)\n",
    "    print(r.boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae4dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs/detect/train/weights/best.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime 1.24.1 with CUDAExecutionProvider\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164314.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 9.6ms\n",
      "Speed: 0.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164315.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 12.7ms\n",
      "Speed: 0.7ms preprocess, 12.7ms inference, 0.7ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164316.jpg: 640x640 2 eye_openeds, 2 mouth_openeds, 1 face, 10.6ms\n",
      "Speed: 0.7ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164317.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 11.5ms\n",
      "Speed: 0.8ms preprocess, 11.5ms inference, 0.8ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164318.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 11.1ms\n",
      "Speed: 0.7ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_164319.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 10.5ms\n",
      "Speed: 0.5ms preprocess, 10.5ms inference, 0.8ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170115.jpg: 640x640 2 eye_closeds, 1 mouth_closed, 1 face, 8.8ms\n",
      "Speed: 0.6ms preprocess, 8.8ms inference, 0.8ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170116.jpg: 640x640 2 eye_openeds, 2 eye_closeds, 1 mouth_closed, 2 faces, 10.1ms\n",
      "Speed: 0.5ms preprocess, 10.1ms inference, 1.1ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170117.jpg: 640x640 2 eye_closeds, 1 mouth_closed, 1 face, 11.6ms\n",
      "Speed: 0.5ms preprocess, 11.6ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170118.jpg: 640x640 2 eye_closeds, 1 mouth_opened, 1 mouth_closed, 1 face, 9.6ms\n",
      "Speed: 0.6ms preprocess, 9.6ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170119.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 1 face, 9.6ms\n",
      "Speed: 0.5ms preprocess, 9.6ms inference, 0.9ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170120.jpg: 640x640 2 eye_closeds, 1 mouth_opened, 1 mouth_closed, 1 face, 8.2ms\n",
      "Speed: 0.5ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170121.jpg: 640x640 2 eye_closeds, 1 mouth_closed, 1 face, 8.7ms\n",
      "Speed: 0.6ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170122.jpg: 640x640 2 eye_closeds, 1 mouth_closed, 1 face, 9.9ms\n",
      "Speed: 0.7ms preprocess, 9.9ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170123.jpg: 640x640 2 eye_openeds, 1 mouth_closed, 1 face, 8.6ms\n",
      "Speed: 0.5ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n",
      "\n",
      "image 1/1 f:\\Study\\cv\\test_photo\\20260212_170124.jpg: 640x640 2 eye_openeds, 1 mouth_opened, 1 face, 9.4ms\n",
      "Speed: 0.5ms preprocess, 9.4ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\test_photo\\result\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import glob\n",
    "\n",
    "test_photos = glob.glob('test_photo/*.jpg')\n",
    "\n",
    "model = YOLO('runs/detect/train/weights/best.onnx')\n",
    "\n",
    "for photo in test_photos:\n",
    "    result = model.predict(photo, conf=0.25, save=True, exist_ok=True, name='result', project='test_photo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acbf70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.13  Python-3.11.2 torch-2.6.0+cu124 CPU (12th Gen Intel Core i5-12400F)\n",
      "YOLO26n summary (fused): 122 layers, 2,375,523 parameters, 0 gradients, 5.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train\\weights\\best.pt' with input shape (1, 1, 640, 640) BCHW and output shape(s) (1, 300, 6) (5.1 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['openvino>=2024.0.0'] not found, attempting AutoUpdate...\n",
      "Collecting openvino>=2024.0.0\n",
      "  Downloading openvino-2025.4.1-20426-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.4.0,>=1.16.6 in .\\.venv\\Lib\\site-packages (from openvino>=2024.0.0) (2.1.3)\n",
      "Collecting openvino-telemetry>=2023.2.1 (from openvino>=2024.0.0)\n",
      "  Downloading openvino_telemetry-2025.2.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: packaging in .\\.venv\\Lib\\site-packages (from openvino>=2024.0.0) (26.0)\n",
      "Downloading openvino-2025.4.1-20426-cp311-cp311-win_amd64.whl (41.8 MB)\n",
      "   ---------------------------------------- 0.0/41.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 7.3/41.8 MB 41.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 18.4/41.8 MB 48.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 29.1/41.8 MB 49.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.7/41.8 MB 53.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.8/41.8 MB 53.2 MB/s  0:00:00\n",
      "Downloading openvino_telemetry-2025.2.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: openvino-telemetry, openvino\n",
      "\n",
      "   ---------------------------------------- 0/2 [openvino-telemetry]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   -------------------- ------------------- 1/2 [openvino]\n",
      "   ---------------------------------------- 2/2 [openvino]\n",
      "\n",
      "Successfully installed openvino-2025.4.1 openvino-telemetry-2025.2.0\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  11.7s\n",
      "WARNING \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2025.4.1-20426-82bbf0292c5-releases/2025/4...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success  24.7s, saved as 'runs\\detect\\train\\weights\\best_openvino_model\\' (9.5 MB)\n",
      "\n",
      "Export complete (25.1s)\n",
      "Results saved to \u001b[1mF:\\Study\\cv\\runs\\detect\\train\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train\\weights\\best_openvino_model imgsz=640 \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train\\weights\\best_openvino_model imgsz=640 data=f:\\Study\\cv\\cv_XIDENT\\data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "\n",
    "model_onnx_320 = model.export(format='OpenVINO', name='best_OpenVINO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
