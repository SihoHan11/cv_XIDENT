{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.32-py3-none-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: absl-py~=2.3 in .\\.venv\\Lib\\site-packages (from mediapipe) (2.4.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from mediapipe) (2.3.5)\n",
      "Requirement already satisfied: sounddevice~=0.5 in .\\.venv\\Lib\\site-packages (from mediapipe) (0.5.5)\n",
      "Requirement already satisfied: flatbuffers~=25.9 in .\\.venv\\Lib\\site-packages (from mediapipe) (25.12.19)\n",
      "Requirement already satisfied: opencv-contrib-python in .\\.venv\\Lib\\site-packages (from mediapipe) (4.13.0.92)\n",
      "Requirement already satisfied: matplotlib in .\\.venv\\Lib\\site-packages (from mediapipe) (3.10.8)\n",
      "Requirement already satisfied: cffi in .\\.venv\\Lib\\site-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in .\\.venv\\Lib\\site-packages (from cffi->sounddevice~=0.5->mediapipe) (3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Using cached mediapipe-0.10.32-py3-none-win_amd64.whl (10.2 MB)\n",
      "Installing collected packages: mediapipe\n",
      "Successfully installed mediapipe-0.10.32\n"
     ]
    }
   ],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a63553",
   "metadata": {},
   "source": [
    "**Level 1 (가벼운 경고)**: \n",
    "- 조건: 하품 1회 발생 OR 눈 1.5초 감김 1회 발생 OR 고개 떨굼 1회 발생\n",
    "- 작동: 경고 알림음\n",
    "\n",
    "**Level 2 (적극적 개입)**: \n",
    "- 조건: 최근 5분 이내에 Level 1 이벤트가 3회 누적되거나, 3분 이내 하품 2회 발생 시 OR 고개 떨굼 2회 발생 시.\n",
    "- 작동: 날카로운 경고음, 음성 출력(예: \"잠시 쉬었다가 운전하세요\")\n",
    "\n",
    "**Level 3 (긴급 솔루션 작동)**: \n",
    "- 조건: 눈을 3초 이상 감고 있거나, 최근 10분 이내에 Level 2가 2회 이상 발생했을 때.\n",
    "- 작동: 최상위 졸음 방지 솔루션 가동. (문제 풀 때까지 지속되는 알림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace11e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mediapipe processing time:  16\n",
      "Loading runs/detect/train_100k/weights/best_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on GPU.0...\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 19.4ms\n",
      "Speed: 1.1ms preprocess, 19.4ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.4ms\n",
      "Speed: 0.7ms preprocess, 17.4ms inference, 1.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  22\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.7ms\n",
      "Speed: 0.4ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.7ms\n",
      "Speed: 1.0ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.8ms\n",
      "Speed: 1.0ms preprocess, 16.8ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  23\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 17.1ms\n",
      "Speed: 0.7ms preprocess, 17.1ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.8ms\n",
      "Speed: 0.6ms preprocess, 15.8ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  33\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.2ms\n",
      "Speed: 0.8ms preprocess, 15.2ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.5ms\n",
      "Speed: 0.8ms preprocess, 16.5ms inference, 1.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.7ms\n",
      "Speed: 0.7ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  31\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.9ms\n",
      "Speed: 0.8ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  27\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.4ms\n",
      "Speed: 0.8ms preprocess, 17.4ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  27\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.3ms\n",
      "Speed: 0.7ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  33\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.6ms\n",
      "Speed: 1.2ms preprocess, 15.6ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  32\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.0ms\n",
      "Speed: 0.6ms preprocess, 16.0ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.1ms\n",
      "Speed: 0.6ms preprocess, 16.1ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.3ms\n",
      "Speed: 0.6ms preprocess, 15.3ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.3ms\n",
      "Speed: 0.4ms preprocess, 15.3ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 14.9ms\n",
      "Speed: 0.6ms preprocess, 14.9ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  24\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.7ms\n",
      "Speed: 0.9ms preprocess, 16.7ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  33\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.6ms\n",
      "Speed: 0.9ms preprocess, 17.6ms inference, 1.2ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  27\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.5ms\n",
      "Speed: 0.9ms preprocess, 16.5ms inference, 0.8ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.4ms\n",
      "Speed: 0.7ms preprocess, 16.4ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  23\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.9ms\n",
      "Speed: 0.4ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  40\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.6ms\n",
      "Speed: 0.5ms preprocess, 16.6ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 17.5ms\n",
      "Speed: 0.7ms preprocess, 17.5ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 19.2ms\n",
      "Speed: 0.6ms preprocess, 19.2ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  40\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 19.6ms\n",
      "Speed: 1.2ms preprocess, 19.6ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.9ms\n",
      "Speed: 0.6ms preprocess, 16.9ms inference, 0.8ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  21\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 14.6ms\n",
      "Speed: 0.5ms preprocess, 14.6ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  36\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.5ms\n",
      "Speed: 0.8ms preprocess, 17.5ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  31\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.6ms\n",
      "Speed: 1.0ms preprocess, 16.6ms inference, 0.9ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  34\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.2ms\n",
      "Speed: 0.6ms preprocess, 16.2ms inference, 0.2ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  34\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.8ms\n",
      "Speed: 1.0ms preprocess, 16.8ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 14.7ms\n",
      "Speed: 0.4ms preprocess, 14.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  20\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 18.4ms\n",
      "Speed: 0.8ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  35\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 15.6ms\n",
      "Speed: 0.5ms preprocess, 15.6ms inference, 0.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.2ms\n",
      "Speed: 0.7ms preprocess, 15.2ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.7ms\n",
      "Speed: 0.7ms preprocess, 16.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  34\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.4ms\n",
      "Speed: 0.6ms preprocess, 16.4ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  42\n",
      "\n",
      "0: 320x320 3 eye_openeds, 1 mouth_opened, 1 mouth_closed, 19.4ms\n",
      "Speed: 0.8ms preprocess, 19.4ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.4ms\n",
      "Speed: 0.7ms preprocess, 15.4ms inference, 1.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  23\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 14.6ms\n",
      "Speed: 0.6ms preprocess, 14.6ms inference, 0.8ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  29\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.7ms\n",
      "Speed: 0.5ms preprocess, 16.7ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  27\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.5ms\n",
      "Speed: 0.5ms preprocess, 16.5ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.2ms\n",
      "Speed: 0.7ms preprocess, 16.2ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  32\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 18.6ms\n",
      "Speed: 1.1ms preprocess, 18.6ms inference, 0.8ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  38\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.9ms\n",
      "Speed: 0.7ms preprocess, 16.9ms inference, 1.1ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.4ms\n",
      "Speed: 0.6ms preprocess, 16.4ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.5ms\n",
      "Speed: 1.2ms preprocess, 15.5ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.4ms\n",
      "Speed: 0.7ms preprocess, 16.4ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  38\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.2ms\n",
      "Speed: 0.8ms preprocess, 15.2ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.4ms\n",
      "Speed: 0.4ms preprocess, 15.4ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  24\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 eye_closed, 1 mouth_closed, 16.6ms\n",
      "Speed: 0.7ms preprocess, 16.6ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  36\n",
      "\n",
      "0: 320x320 1 eye_opened, 1 eye_closed, 1 mouth_closed, 16.9ms\n",
      "Speed: 0.9ms preprocess, 16.9ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  37\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 19.3ms\n",
      "Speed: 1.0ms preprocess, 19.3ms inference, 0.9ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.5ms\n",
      "Speed: 0.5ms preprocess, 15.5ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  35\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 16.2ms\n",
      "Speed: 1.0ms preprocess, 16.2ms inference, 0.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  36\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 16.3ms\n",
      "Speed: 0.8ms preprocess, 16.3ms inference, 1.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.3ms\n",
      "Speed: 0.8ms preprocess, 16.3ms inference, 0.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  38\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.4ms\n",
      "Speed: 1.4ms preprocess, 17.4ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  29\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.3ms\n",
      "Speed: 0.7ms preprocess, 17.3ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  31\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.9ms\n",
      "Speed: 1.0ms preprocess, 16.9ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 15.7ms\n",
      "Speed: 0.5ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  45\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.8ms\n",
      "Speed: 1.1ms preprocess, 17.8ms inference, 0.9ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  24\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.6ms\n",
      "Speed: 0.3ms preprocess, 15.6ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 18.2ms\n",
      "Speed: 0.9ms preprocess, 18.2ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  29\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.3ms\n",
      "Speed: 0.7ms preprocess, 16.3ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  35\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 18.0ms\n",
      "Speed: 0.7ms preprocess, 18.0ms inference, 0.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  33\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.8ms\n",
      "Speed: 0.6ms preprocess, 15.8ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  35\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 18.4ms\n",
      "Speed: 1.1ms preprocess, 18.4ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  26\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.4ms\n",
      "Speed: 0.6ms preprocess, 16.4ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  19\n",
      "\n",
      "0: 320x320 3 eye_openeds, 1 mouth_closed, 16.3ms\n",
      "Speed: 0.4ms preprocess, 16.3ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  24\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.9ms\n",
      "Speed: 0.3ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  34\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.7ms\n",
      "Speed: 0.9ms preprocess, 15.7ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  32\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.9ms\n",
      "Speed: 0.7ms preprocess, 15.9ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.5ms\n",
      "Speed: 0.6ms preprocess, 16.5ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  30\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 18.1ms\n",
      "Speed: 1.2ms preprocess, 18.1ms inference, 1.0ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  25\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.7ms\n",
      "Speed: 0.4ms preprocess, 16.7ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  32\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.5ms\n",
      "Speed: 0.7ms preprocess, 16.5ms inference, 0.5ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  27\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.3ms\n",
      "Speed: 0.6ms preprocess, 16.3ms inference, 0.3ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  40\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 16.3ms\n",
      "Speed: 0.9ms preprocess, 16.3ms inference, 0.7ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  33\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_opened, 1 mouth_closed, 16.5ms\n",
      "Speed: 1.2ms preprocess, 16.5ms inference, 0.4ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  35\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 15.9ms\n",
      "Speed: 1.1ms preprocess, 15.9ms inference, 0.6ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n",
      "mediapipe processing time:  28\n",
      "\n",
      "0: 320x320 2 eye_openeds, 1 mouth_closed, 17.1ms\n",
      "Speed: 0.3ms preprocess, 17.1ms inference, 1.0ms postprocess per image at shape (1, 1, 320, 320)\n",
      "[False, False, False]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         \u001b[38;5;66;03m# 0.3초 간격 대기\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# 자원 해제 및 윈도우 닫기\u001b[39;00m\n\u001b[32m     93\u001b[39m     cap.release()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from utils.camPredictUtils import *\n",
    "from utils.mediapipeUtils import *\n",
    "\n",
    "model = YOLO(\"runs/detect/train_100k/weights/best_openvino_model\")\n",
    "\n",
    "# 웹캠 연결 (0번 카메라)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# FaceProcessor 클래스 인스턴스 생성\n",
    "mediapipeProcess = FaceProcessor()\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        isEyeClosed = False\n",
    "        isYawn = False\n",
    "        isHeadDrop = False\n",
    "        \n",
    "        # 프레임 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        color_frame = frame.copy()  # 테스트 시각화용 컬러 프레임 복제\n",
    "\n",
    "        # 고개떨굼 여부, 얼굴 범위, 얼굴 주변 크롭할 범위 받아오기\n",
    "        mpProcessed = mediapipeProcess.process_frame(frame)\n",
    "        if mpProcessed == None :\n",
    "            # return None\n",
    "            continue\n",
    "        isHeadDrop, bbox_coords, crop_coords, cropped_frame = mpProcessed\n",
    "        frame = cv2.cvtColor(cropped_frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # cv2.imshow(\"YOLO Real-time Inference\", frame)\n",
    "\n",
    "        # 모델 추론 수행 (verbose=False로 로그 출력 억제)\n",
    "        results = model.predict(\n",
    "            source=frame,\n",
    "            verbose=False,\n",
    "            save=False,\n",
    "            imgsz=320,\n",
    "            conf=0.25,\n",
    "            # agnostic_nms=True,\n",
    "            # iou=0.3,\n",
    "            classes=[0, 1, 2, 3],       # Face는 mediapipe로 추출\n",
    "            device=\"intel:gpu\"\n",
    "            )\n",
    "        \n",
    "        # 결과 시각화 프레임 생성\n",
    "        # annotated_frame = results[0].plot()\n",
    "        filtered_data = filter_overlapping_parts(results)\n",
    "\n",
    "        annotated_frame = draw_face_box(color_frame, bbox_coords, isHeadDrop)\n",
    "        annotated_frame = draw_filtered_results(color_frame, filtered_data, ['eye_opened', 'eye_closed', 'mouth_opened', 'mouth_closed'], crop_coords)\n",
    "\n",
    "        # 눈 감김 여부, 하품 여부 확인\n",
    "        # print(filtered_data)\n",
    "        closed_eye_count = 0\n",
    "        for detected in filtered_data :\n",
    "            if detected[5] == 1 :\n",
    "                closed_eye_count += 1\n",
    "            elif detected[5] == 2 and isYawning(detected, bbox_coords):\n",
    "                isYawn = True           # 하품 감지\n",
    "\n",
    "        # 눈 둘 다 감겨있으면 감은 눈으로 판정\n",
    "        if closed_eye_count >= 2 :\n",
    "            isEyeClosed = True\n",
    "        \n",
    "        '''\n",
    "        리턴할 리스트\n",
    "        Index 0 (하품): 현재 프레임에서 입의 벌어짐 감지 여부\n",
    "        Index 1 (눈 감김): 현재 프레임에서 안구 폐쇄 감지 여부\n",
    "        Index 2 (고개 떨굼): 현재 프레임에서 두부의 하향 굴곡 감지 여부\n",
    "        '''\n",
    "        detectedList = [isYawn, isEyeClosed, isHeadDrop]\n",
    "        print(detectedList)\n",
    "        \n",
    "        # 실시간 시각화 창 표시\n",
    "        cv2.imshow(\"YOLO Real-time Inference\", annotated_frame)\n",
    "\n",
    "        # 'q' 키를 누르면 루프 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # 0.3초 간격 대기\n",
    "        time.sleep(0.3)\n",
    "finally:\n",
    "    # 자원 해제 및 윈도우 닫기\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
