{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41803f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.32-py3-none-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: absl-py~=2.3 in .\\.venv\\Lib\\site-packages (from mediapipe) (2.4.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from mediapipe) (2.3.5)\n",
      "Requirement already satisfied: sounddevice~=0.5 in .\\.venv\\Lib\\site-packages (from mediapipe) (0.5.5)\n",
      "Requirement already satisfied: flatbuffers~=25.9 in .\\.venv\\Lib\\site-packages (from mediapipe) (25.12.19)\n",
      "Requirement already satisfied: opencv-contrib-python in .\\.venv\\Lib\\site-packages (from mediapipe) (4.13.0.92)\n",
      "Requirement already satisfied: matplotlib in .\\.venv\\Lib\\site-packages (from mediapipe) (3.10.8)\n",
      "Requirement already satisfied: cffi in .\\.venv\\Lib\\site-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in .\\.venv\\Lib\\site-packages (from cffi->sounddevice~=0.5->mediapipe) (3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\.venv\\Lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Using cached mediapipe-0.10.32-py3-none-win_amd64.whl (10.2 MB)\n",
      "Installing collected packages: mediapipe\n",
      "Successfully installed mediapipe-0.10.32\n"
     ]
    }
   ],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a63553",
   "metadata": {},
   "source": [
    "**Level 1 (가벼운 경고)**: \n",
    "- 조건: 하품 1회 발생 OR 눈 1.5초 감김 1회 발생 OR 고개 떨굼 1회 발생\n",
    "- 작동: 경고 알림음\n",
    "\n",
    "**Level 2 (적극적 개입)**: \n",
    "- 조건: 최근 5분 이내에 Level 1 이벤트가 3회 누적되거나, 3분 이내 하품 2회 발생 시 OR 고개 떨굼 2회 발생 시.\n",
    "- 작동: 날카로운 경고음, 음성 출력(예: \"잠시 쉬었다가 운전하세요\")\n",
    "\n",
    "**Level 3 (긴급 솔루션 작동)**: \n",
    "- 조건: 눈을 3초 이상 감고 있거나, 최근 10분 이내에 Level 2가 2회 이상 발생했을 때.\n",
    "- 작동: 최상위 졸음 방지 솔루션 가동. (문제 풀 때까지 지속되는 알림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace11e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runs/detect/train_100k/weights/best_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference on GPU.0...\n",
      "\n",
      "0: 640x640 3 eye_openeds, 33.3ms\n",
      "Speed: 1.1ms preprocess, 33.3ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.9ms\n",
      "Speed: 1.8ms preprocess, 29.9ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 1 mouth_closed, 30.3ms\n",
      "Speed: 1.6ms preprocess, 30.3ms inference, 0.6ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.6ms\n",
      "Speed: 1.1ms preprocess, 29.6ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 mouth_closed, 29.5ms\n",
      "Speed: 1.0ms preprocess, 29.5ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 30.0ms\n",
      "Speed: 0.9ms preprocess, 30.0ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 4 eye_openeds, 1 mouth_closed, 29.7ms\n",
      "Speed: 0.9ms preprocess, 29.7ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 2 eye_closeds, 1 mouth_closed, 30.0ms\n",
      "Speed: 1.2ms preprocess, 30.0ms inference, 0.6ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 1 mouth_closed, 30.9ms\n",
      "Speed: 1.0ms preprocess, 30.9ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 2 mouth_closeds, 29.4ms\n",
      "Speed: 1.6ms preprocess, 29.4ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 eye_opened, 2 eye_closeds, 29.8ms\n",
      "Speed: 1.2ms preprocess, 29.8ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 eye_closed, 1 mouth_closed, 29.7ms\n",
      "Speed: 1.6ms preprocess, 29.7ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.6ms\n",
      "Speed: 1.0ms preprocess, 29.6ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 30.5ms\n",
      "Speed: 1.1ms preprocess, 30.5ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.7ms\n",
      "Speed: 0.8ms preprocess, 29.7ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.9ms\n",
      "Speed: 1.3ms preprocess, 29.9ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 30.0ms\n",
      "Speed: 1.0ms preprocess, 30.0ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 1 mouth_closed, 29.5ms\n",
      "Speed: 1.1ms preprocess, 29.5ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.3ms\n",
      "Speed: 0.9ms preprocess, 29.3ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 eye_opened, 30.0ms\n",
      "Speed: 1.0ms preprocess, 30.0ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 1 mouth_opened, 30.1ms\n",
      "Speed: 1.1ms preprocess, 30.1ms inference, 0.4ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 eye_opened, 30.0ms\n",
      "Speed: 1.0ms preprocess, 30.0ms inference, 0.5ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 2 eye_openeds, 29.4ms\n",
      "Speed: 0.8ms preprocess, 29.4ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n",
      "\n",
      "0: 640x640 1 eye_opened, 1 eye_closed, 1 mouth_opened, 29.5ms\n",
      "Speed: 0.9ms preprocess, 29.5ms inference, 0.3ms postprocess per image at shape (1, 1, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from utils.camPredictUtils import *\n",
    "from utils.mediapipeUtils import *\n",
    "\n",
    "model = YOLO(\"runs/detect/train_100k/weights/best_openvino_model\")\n",
    "\n",
    "# 웹캠 연결 (0번 카메라)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# FaceProcessor 클래스 인스턴스 생성\n",
    "mediapipeProcess = FaceProcessor()\n",
    "\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 프레임 좌우 반전\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        color_frame = frame.copy()\n",
    "        head_drop, bbox_coords, crop_coords = mediapipeProcess.process_frame(frame)\n",
    "        cropped_frame = frame[crop_coords[1]:crop_coords[3], crop_coords[0]:crop_coords[2]]\n",
    "        frame = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # cv2.imshow(\"YOLO Real-time Inference\", frame)\n",
    "\n",
    "        # 모델 추론 수행 (verbose=False로 로그 출력 억제)\n",
    "        results = model.predict(\n",
    "            source=frame,\n",
    "            verbose=True,\n",
    "            save=False,\n",
    "            imgsz=640,\n",
    "            conf=0.25,\n",
    "            agnostic_nms=True,\n",
    "            iou=0.3,\n",
    "            classes=[0, 1, 2, 3],       # Face는 mediapipe로 추출\n",
    "            device=\"intel:gpu\"\n",
    "            )\n",
    "        \n",
    "        # 결과 시각화 프레임 생성\n",
    "        # annotated_frame = results[0].plot()\n",
    "        filtered_data = filter_overlapping_parts(results)\n",
    "\n",
    "        annotated_frame = draw_face_box(color_frame, bbox_coords, head_drop)\n",
    "        annotated_frame = draw_filtered_results(color_frame, filtered_data, ['eye_opened', 'eye_closed', 'mouth_opened', 'mouth_closed'], crop_coords)\n",
    "        \n",
    "        # 실시간 시각화 창 표시\n",
    "        cv2.imshow(\"YOLO Real-time Inference\", annotated_frame)\n",
    "\n",
    "        # 'q' 키를 누르면 루프 종료\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # 0.2초 간격 대기\n",
    "        time.sleep(0.3)\n",
    "finally:\n",
    "    # 자원 해제 및 윈도우 닫기\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
